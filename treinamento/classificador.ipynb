{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJbFuG-Ugv02"
   },
   "source": [
    "# Classificador\n",
    "\n",
    "Neste Workshop, vamos aprender como criar um classificador que diz quais músicas são da Rihanna e quais são da Beyoncé utilizando aprendizado de máquina, mais especificamente *Aprendizado Supervisionado*, uma das áreas de Machine Learning.\n",
    "\n",
    "Então vamos lá!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Qqq3M_vPW2p"
   },
   "source": [
    "## Importando os dados\n",
    "\n",
    "O primeiro passo é importar seus dados, no caso nosso Dataframe (como chamamos a *tabela* que guarda as informações que usaremos). Fazemos isso com a biblioteca `pandas`.\n",
    "\n",
    "Com a função `read_csv` lemos nosso arquivo e guardamos ele na váriavel `df`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DC8V9WPPW2w"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dv9XVp4NPW25",
    "outputId": "db1785f4-8c19-4e92-d683-ad6ace15c439"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nome da Música</th>\n",
       "      <th>link</th>\n",
       "      <th>album</th>\n",
       "      <th>letra</th>\n",
       "      <th>artista</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'03 Bonnie &amp; Clyde</td>\n",
       "      <td>/beyonce/03-bonnie-clyde.html</td>\n",
       "      <td>I Am... Yours: An Intimate Performance at Wynn...</td>\n",
       "      <td>Jay-z Uh-uh-uh You ready b? Let's go get 'em. ...</td>\n",
       "      <td>Beyoncé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>***Flawless (Feat. Chimamanda Ngozi Adichie)</td>\n",
       "      <td>/beyonce/flawless-feat-chimamanda-ngozi-adichi...</td>\n",
       "      <td>BEYONCÉ</td>\n",
       "      <td>Your challengers are a young group from Housto...</td>\n",
       "      <td>Beyoncé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***Flawless (Feat. Nicki Minaj)</td>\n",
       "      <td>/beyonce/flawless-feat-nicki-minaj.html</td>\n",
       "      <td>BEYONCÉ [Platinum Edition]</td>\n",
       "      <td>Dum-da-de-da Do, do, do, do, do, do (Coming do...</td>\n",
       "      <td>Beyoncé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1+1</td>\n",
       "      <td>/beyonce/11.html</td>\n",
       "      <td>BEYONCÉ [Platinum Edition]</td>\n",
       "      <td>If I ain't got nothing I got you If I ain't go...</td>\n",
       "      <td>Beyoncé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6 Inch (Feat. The Weeknd)</td>\n",
       "      <td>/beyonce/6-inch-feat-the-weeknd.html</td>\n",
       "      <td>LEMONADE</td>\n",
       "      <td>Six inch heels She walked in the club like nob...</td>\n",
       "      <td>Beyoncé</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Nome da Música  \\\n",
       "0                            '03 Bonnie & Clyde   \n",
       "1  ***Flawless (Feat. Chimamanda Ngozi Adichie)   \n",
       "2               ***Flawless (Feat. Nicki Minaj)   \n",
       "3                                           1+1   \n",
       "4                     6 Inch (Feat. The Weeknd)   \n",
       "\n",
       "                                                link  \\\n",
       "0                      /beyonce/03-bonnie-clyde.html   \n",
       "1  /beyonce/flawless-feat-chimamanda-ngozi-adichi...   \n",
       "2            /beyonce/flawless-feat-nicki-minaj.html   \n",
       "3                                   /beyonce/11.html   \n",
       "4               /beyonce/6-inch-feat-the-weeknd.html   \n",
       "\n",
       "                                               album  \\\n",
       "0  I Am... Yours: An Intimate Performance at Wynn...   \n",
       "1                                            BEYONCÉ   \n",
       "2                         BEYONCÉ [Platinum Edition]   \n",
       "3                         BEYONCÉ [Platinum Edition]   \n",
       "4                                           LEMONADE   \n",
       "\n",
       "                                               letra  artista  \n",
       "0  Jay-z Uh-uh-uh You ready b? Let's go get 'em. ...  Beyoncé  \n",
       "1  Your challengers are a young group from Housto...  Beyoncé  \n",
       "2  Dum-da-de-da Do, do, do, do, do, do (Coming do...  Beyoncé  \n",
       "3  If I ain't got nothing I got you If I ain't go...  Beyoncé  \n",
       "4  Six inch heels She walked in the club like nob...  Beyoncé  "
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('beyonce_rihanna.csv', index_col=0)\n",
    "\n",
    "# vamos explorar nosso dataframe olhando apenas as primeiras linhas com a função abaixo:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a63DRicqPW3G"
   },
   "source": [
    "## Pré-processamentos\n",
    "\n",
    "Antes de partir para o aprendizado de máquina, precisamos preparar nosso texto. Fazemos isso porque, para a máquina, algumas palavras ou estruturas do nosso texto não importam e não fazem diferença. \n",
    "São muitos os métodos de pré-processamento, mas aqui vamos realizar apenas alguns: \n",
    "* Tokenização\n",
    "* Remover stopwords\n",
    "* Deixar todo o texto em minúsculo\n",
    "* Selecionar apenas letras com REGEX\n",
    "* Lemmatização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "poFLBHOcPW3J",
    "outputId": "a74b3c99-759e-4621-b88a-5a40b534dc40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here I am Looking in the mirror An open face, the pain erased Now the sky is clearer I can see the sun Now that all is, all is said and done, oh  There you are Always strong when I need you You let me give And now I live, fearless and protected With the one I will love After all is, all is said and done  I once believed that hearts were made to bleed (Inside I once believed that hearts were made to bleed, oh baby) But now I'm not afraid to say I need you, I need you so stay with me  These precious (precious) hours (yeah) Greet each dawn in open arms And dream, into tomorrow  Where there's only love After all is, all is said and done  (Yeah baby) Oh baby (Inside I once believed, That hearts were meant to bleed)  (I'll never) I'll never be afraid to say I need you, I need you, so here  Here we are in the still of this moment Fear is gone, hope lives on  We found our happy ending For there's only love (only love) And this sweet, sweet love After all is, all is said and done  Yeah baby after all is (all is)  All is said and done\""
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilizando uma música como exemplo\n",
    "exemplo = df['letra'][10]\n",
    "exemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dY_Ku-a_PW3b"
   },
   "source": [
    "### Tokenização \n",
    "\n",
    "Uma parte importante no pré-processamento de um texto é a tokenização. Isto é, transformar elementos do seu texto em tokens, ou seja, strings dentro de uma lista  -  ou, se você não tiver conhecimento de python, transformar todas as palavras do texto em elementos individuais separados por aspas. \n",
    "Podemos tokenizar palavras com `word_tokenize`, essa função recebe o texto como argumento e retorna todas as palavras do texto em forma de tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vcXQSNTOPW3c"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgTkezQEPW3n",
    "outputId": "776a61d5-5363-4c0c-b4e4-a7691ad4b5ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Looking',\n",
       " 'in',\n",
       " 'the',\n",
       " 'mirror',\n",
       " 'An',\n",
       " 'open',\n",
       " 'face',\n",
       " ',',\n",
       " 'the',\n",
       " 'pain',\n",
       " 'erased',\n",
       " 'Now',\n",
       " 'the',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'clearer',\n",
       " 'I',\n",
       " 'can',\n",
       " 'see',\n",
       " 'the',\n",
       " 'sun',\n",
       " 'Now',\n",
       " 'that',\n",
       " 'all',\n",
       " 'is',\n",
       " ',',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done',\n",
       " ',',\n",
       " 'oh',\n",
       " 'There',\n",
       " 'you',\n",
       " 'are',\n",
       " 'Always',\n",
       " 'strong',\n",
       " 'when',\n",
       " 'I',\n",
       " 'need',\n",
       " 'you',\n",
       " 'You',\n",
       " 'let',\n",
       " 'me',\n",
       " 'give',\n",
       " 'And',\n",
       " 'now',\n",
       " 'I',\n",
       " 'live',\n",
       " ',',\n",
       " 'fearless',\n",
       " 'and',\n",
       " 'protected',\n",
       " 'With',\n",
       " 'the',\n",
       " 'one',\n",
       " 'I',\n",
       " 'will',\n",
       " 'love',\n",
       " 'After',\n",
       " 'all',\n",
       " 'is',\n",
       " ',',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done',\n",
       " 'I',\n",
       " 'once',\n",
       " 'believed',\n",
       " 'that',\n",
       " 'hearts',\n",
       " 'were',\n",
       " 'made',\n",
       " 'to',\n",
       " 'bleed',\n",
       " '(',\n",
       " 'Inside',\n",
       " 'I',\n",
       " 'once',\n",
       " 'believed',\n",
       " 'that',\n",
       " 'hearts',\n",
       " 'were',\n",
       " 'made',\n",
       " 'to',\n",
       " 'bleed',\n",
       " ',',\n",
       " 'oh',\n",
       " 'baby',\n",
       " ')',\n",
       " 'But',\n",
       " 'now',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'not',\n",
       " 'afraid',\n",
       " 'to',\n",
       " 'say',\n",
       " 'I',\n",
       " 'need',\n",
       " 'you',\n",
       " ',',\n",
       " 'I',\n",
       " 'need',\n",
       " 'you',\n",
       " 'so',\n",
       " 'stay',\n",
       " 'with',\n",
       " 'me',\n",
       " 'These',\n",
       " 'precious',\n",
       " '(',\n",
       " 'precious',\n",
       " ')',\n",
       " 'hours',\n",
       " '(',\n",
       " 'yeah',\n",
       " ')',\n",
       " 'Greet',\n",
       " 'each',\n",
       " 'dawn',\n",
       " 'in',\n",
       " 'open',\n",
       " 'arms',\n",
       " 'And',\n",
       " 'dream',\n",
       " ',',\n",
       " 'into',\n",
       " 'tomorrow',\n",
       " 'Where',\n",
       " 'there',\n",
       " \"'s\",\n",
       " 'only',\n",
       " 'love',\n",
       " 'After',\n",
       " 'all',\n",
       " 'is',\n",
       " ',',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done',\n",
       " '(',\n",
       " 'Yeah',\n",
       " 'baby',\n",
       " ')',\n",
       " 'Oh',\n",
       " 'baby',\n",
       " '(',\n",
       " 'Inside',\n",
       " 'I',\n",
       " 'once',\n",
       " 'believed',\n",
       " ',',\n",
       " 'That',\n",
       " 'hearts',\n",
       " 'were',\n",
       " 'meant',\n",
       " 'to',\n",
       " 'bleed',\n",
       " ')',\n",
       " '(',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'never',\n",
       " ')',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'never',\n",
       " 'be',\n",
       " 'afraid',\n",
       " 'to',\n",
       " 'say',\n",
       " 'I',\n",
       " 'need',\n",
       " 'you',\n",
       " ',',\n",
       " 'I',\n",
       " 'need',\n",
       " 'you',\n",
       " ',',\n",
       " 'so',\n",
       " 'here',\n",
       " 'Here',\n",
       " 'we',\n",
       " 'are',\n",
       " 'in',\n",
       " 'the',\n",
       " 'still',\n",
       " 'of',\n",
       " 'this',\n",
       " 'moment',\n",
       " 'Fear',\n",
       " 'is',\n",
       " 'gone',\n",
       " ',',\n",
       " 'hope',\n",
       " 'lives',\n",
       " 'on',\n",
       " 'We',\n",
       " 'found',\n",
       " 'our',\n",
       " 'happy',\n",
       " 'ending',\n",
       " 'For',\n",
       " 'there',\n",
       " \"'s\",\n",
       " 'only',\n",
       " 'love',\n",
       " '(',\n",
       " 'only',\n",
       " 'love',\n",
       " ')',\n",
       " 'And',\n",
       " 'this',\n",
       " 'sweet',\n",
       " ',',\n",
       " 'sweet',\n",
       " 'love',\n",
       " 'After',\n",
       " 'all',\n",
       " 'is',\n",
       " ',',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done',\n",
       " 'Yeah',\n",
       " 'baby',\n",
       " 'after',\n",
       " 'all',\n",
       " 'is',\n",
       " '(',\n",
       " 'all',\n",
       " 'is',\n",
       " ')',\n",
       " 'All',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done']"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizando a primeira música\n",
    "tokens = word_tokenize(exemplo)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8iX6icQ4PW34"
   },
   "source": [
    "### Selecionando apenas as letras e deixando todas em minúsculas\n",
    "\n",
    "Para a máquina, pontuações não são necessárias, por isso um pré-processamento necessário é selecionar apenas as letras de um texto. \n",
    "\n",
    "Porém, **antes disso** precisamos deixar todas as letras em minúsculo, não somente porque isso facilita a aplicação do REGEX, mas também porque a máquina tende a interpretar palavras com letras maiúsculas e minúsculas como sendo diferentes. Por exemplo, Beyoncé e beyoncé podem ser interpretadas como palavras distintas. Então vamos deixar as letras minúsculas com a função `.lower`.\n",
    "\n",
    "Feito isso, podemos selecionar apenas as letras com REGEX, mais especificamente com a função `re.findall`, que, além de retornar apenas as letras, já tokeniza o texto para você! \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWfHh-yfPW38",
    "outputId": "e1653025-f40f-428e-d8e2-7af711870b64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here',\n",
       " 'i',\n",
       " 'am',\n",
       " 'looking',\n",
       " 'in',\n",
       " 'the',\n",
       " 'mirror',\n",
       " 'an',\n",
       " 'open',\n",
       " 'face',\n",
       " 'the',\n",
       " 'pain',\n",
       " 'erased',\n",
       " 'now',\n",
       " 'the',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'clearer',\n",
       " 'i',\n",
       " 'can',\n",
       " 'see',\n",
       " 'the',\n",
       " 'sun',\n",
       " 'now',\n",
       " 'that',\n",
       " 'all',\n",
       " 'is',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done',\n",
       " 'oh',\n",
       " 'there',\n",
       " 'you',\n",
       " 'are',\n",
       " 'always',\n",
       " 'strong',\n",
       " 'when',\n",
       " 'i',\n",
       " 'need',\n",
       " 'you',\n",
       " 'you',\n",
       " 'let',\n",
       " 'me',\n",
       " 'give',\n",
       " 'and',\n",
       " 'now',\n",
       " 'i',\n",
       " 'live',\n",
       " 'fearless',\n",
       " 'and',\n",
       " 'protected',\n",
       " 'with',\n",
       " 'the',\n",
       " 'one',\n",
       " 'i',\n",
       " 'will',\n",
       " 'love',\n",
       " 'after',\n",
       " 'all',\n",
       " 'is',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done',\n",
       " 'i',\n",
       " 'once',\n",
       " 'believed',\n",
       " 'that',\n",
       " 'hearts',\n",
       " 'were',\n",
       " 'made',\n",
       " 'to',\n",
       " 'bleed',\n",
       " 'inside',\n",
       " 'i',\n",
       " 'once',\n",
       " 'believed',\n",
       " 'that',\n",
       " 'hearts',\n",
       " 'were',\n",
       " 'made',\n",
       " 'to',\n",
       " 'bleed',\n",
       " 'oh',\n",
       " 'baby',\n",
       " 'but',\n",
       " 'now',\n",
       " 'i',\n",
       " 'm',\n",
       " 'not',\n",
       " 'afraid',\n",
       " 'to',\n",
       " 'say',\n",
       " 'i',\n",
       " 'need',\n",
       " 'you',\n",
       " 'i',\n",
       " 'need',\n",
       " 'you',\n",
       " 'so',\n",
       " 'stay',\n",
       " 'with',\n",
       " 'me',\n",
       " 'these',\n",
       " 'precious',\n",
       " 'precious',\n",
       " 'hours',\n",
       " 'yeah',\n",
       " 'greet',\n",
       " 'each',\n",
       " 'dawn',\n",
       " 'in',\n",
       " 'open',\n",
       " 'arms',\n",
       " 'and',\n",
       " 'dream',\n",
       " 'into',\n",
       " 'tomorrow',\n",
       " 'where',\n",
       " 'there',\n",
       " 's',\n",
       " 'only',\n",
       " 'love',\n",
       " 'after',\n",
       " 'all',\n",
       " 'is',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done',\n",
       " 'yeah',\n",
       " 'baby',\n",
       " 'oh',\n",
       " 'baby',\n",
       " 'inside',\n",
       " 'i',\n",
       " 'once',\n",
       " 'believed',\n",
       " 'that',\n",
       " 'hearts',\n",
       " 'were',\n",
       " 'meant',\n",
       " 'to',\n",
       " 'bleed',\n",
       " 'i',\n",
       " 'll',\n",
       " 'never',\n",
       " 'i',\n",
       " 'll',\n",
       " 'never',\n",
       " 'be',\n",
       " 'afraid',\n",
       " 'to',\n",
       " 'say',\n",
       " 'i',\n",
       " 'need',\n",
       " 'you',\n",
       " 'i',\n",
       " 'need',\n",
       " 'you',\n",
       " 'so',\n",
       " 'here',\n",
       " 'here',\n",
       " 'we',\n",
       " 'are',\n",
       " 'in',\n",
       " 'the',\n",
       " 'still',\n",
       " 'of',\n",
       " 'this',\n",
       " 'moment',\n",
       " 'fear',\n",
       " 'is',\n",
       " 'gone',\n",
       " 'hope',\n",
       " 'lives',\n",
       " 'on',\n",
       " 'we',\n",
       " 'found',\n",
       " 'our',\n",
       " 'happy',\n",
       " 'ending',\n",
       " 'for',\n",
       " 'there',\n",
       " 's',\n",
       " 'only',\n",
       " 'love',\n",
       " 'only',\n",
       " 'love',\n",
       " 'and',\n",
       " 'this',\n",
       " 'sweet',\n",
       " 'sweet',\n",
       " 'love',\n",
       " 'after',\n",
       " 'all',\n",
       " 'is',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done',\n",
       " 'yeah',\n",
       " 'baby',\n",
       " 'after',\n",
       " 'all',\n",
       " 'is',\n",
       " 'all',\n",
       " 'is',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done']"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "letras = re.findall(r'\\b[A-zÀ-úü]+\\b', exemplo.lower())\n",
    "letras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wn-sfEMSPW4a"
   },
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords são palavras que, apesar de muito frequentes, não são importantes/relevantes para a máquina. Entre elas, podemos encontrar artigos como “o” e “uma”, ou preposições como “de” e “em”, entre outras palavras frequentes no idioma. Para removê-las do texto, utilizamos uma lista de stopwords disponível na biblioteca NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RngDaWfjPW4f",
    "outputId": "9488d6ba-9fd8-4f4e-b0d0-a720d5ebb486"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "stops #lista de stopwords em inglês "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOwQ1ed6PW4n"
   },
   "source": [
    "Como remover stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWI0dceNPW4p",
    "outputId": "0a03e23d-a82f-43ab-d49f-90f0f6a24846"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'looking mirror open face pain erased sky clearer see sun said done oh always strong need let give live fearless protected one love said done believed hearts made bleed inside believed hearts made bleed oh baby afraid say need need stay precious precious hours yeah greet dawn open arms dream tomorrow love said done yeah baby oh baby inside believed hearts meant bleed never never afraid say need need still moment fear gone hope lives found happy ending love love sweet sweet love said done yeah baby said done'"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem_stopwords = [palavra for palavra in letras if palavra not in stops]\n",
    "palavras_importantes = \" \".join(sem_stopwords)\n",
    "palavras_importantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "afGEg33WPW4z"
   },
   "source": [
    "### Lematização \n",
    "\n",
    "Assim como Stopwords, ter verbos conjugados em um texto não faz diferença quando a máquina vai processá-lo. Por isso, existem duas ferramentas chamadas Lemmatização e Stemmatização. Ambas fazem a mesma coisa: Quando passado um texto como argumento, elas reduzem todas as formas verbais conjugadas à sua raiz. A única diferença, entretanto, é que a função que lemmatiza seu texto reduz todos os verbos a forma verdadeira da raiz  -  por isso quanto maior seu texto, mais tempo essa função demora para rodar no código - , enquanto a função que stemmatiza apenas \"corta\" as palavras no meio usando a raiz como base, o que pode gerar palavras que não existem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGXq4zZiPW40"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spc = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSZ772ZqPW5A",
    "outputId": "6e3bf869-021d-4c0c-c424-32aa8c83d32f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look mirror open face pain erase sky clearer see sun say do oh always strong need let give live fearless protect one love say do believe hearts make bleed inside believe hearts make bleed oh baby afraid say need need stay precious precious hours yeah greet dawn open arms dream tomorrow love say do yeah baby oh baby inside believe hearts mean bleed never never afraid say need nee still moment fear go hope lives find happy end love love sweet sweet love say do yeah baby say do\n"
     ]
    }
   ],
   "source": [
    "spc_letras = spc(palavras_importantes)\n",
    "lemmas = [token.lemma_ if token.pos_ == 'VERB' else str(token) for token in spc_letras]\n",
    "texto_limpo = \" \".join(lemmas)\n",
    "print(texto_limpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mYlDvyRSPW5q"
   },
   "source": [
    "Vamos construir uma função para realizar todos os pré-processamentos ao invés de fazê-los um a um: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-yiLgyaPW5y"
   },
   "outputs": [],
   "source": [
    "def limpar_texto(texto):\n",
    "\n",
    "    # selecionando apenas as letras e convertendo para minúscula \n",
    "    letras =  re.findall(r'\\b[A-zÀ-úü]+\\b', texto.lower())\n",
    "    \n",
    "    # removendo as stopwords \n",
    "    stops = set(stopwords.words('portuguese'))\n",
    "    palavras = [w for w in letras if w not in stops]\n",
    "    palavras_importantes = \" \".join(palavras)\n",
    "    \n",
    "    # lematização \n",
    "    spc_letras = spc(palavras_importantes)\n",
    "    lemmas = [token.lemma_ if token.pos_ == 'VERB' else str(token) for token in spc_letras]\n",
    "    texto_limpo = \" \".join(lemmas)\n",
    "    \n",
    "    return texto_limpo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tCW0gFxwpNPL"
   },
   "source": [
    "Agora vamos aplicá-la aos nossos dados, mais espcificamente na coluna \"letra\", que contém as músicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mrrNlaKPW6V"
   },
   "outputs": [],
   "source": [
    "df['Texto Limpo'] = df['letra'].apply(limpar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bI9AD4--PW6p",
    "outputId": "dd613dbf-904b-43ee-e778-a5cbf10777c3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nome da Música</th>\n",
       "      <th>link</th>\n",
       "      <th>album</th>\n",
       "      <th>letra</th>\n",
       "      <th>artista</th>\n",
       "      <th>Texto Limpo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'03 Bonnie &amp; Clyde</td>\n",
       "      <td>/beyonce/03-bonnie-clyde.html</td>\n",
       "      <td>I Am... Yours: An Intimate Performance at Wynn...</td>\n",
       "      <td>Jay-z Uh-uh-uh You ready b? Let's go get 'em. ...</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>jay z uh uh uh you ready b let s go get look y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>***Flawless (Feat. Chimamanda Ngozi Adichie)</td>\n",
       "      <td>/beyonce/flawless-feat-chimamanda-ngozi-adichi...</td>\n",
       "      <td>BEYONCÉ</td>\n",
       "      <td>Your challengers are a young group from Housto...</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>your challengers are young group from houston ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***Flawless (Feat. Nicki Minaj)</td>\n",
       "      <td>/beyonce/flawless-feat-nicki-minaj.html</td>\n",
       "      <td>BEYONCÉ [Platinum Edition]</td>\n",
       "      <td>Dum-da-de-da Do, do, do, do, do, do (Coming do...</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>dum come down drip candy on the ground it stay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1+1</td>\n",
       "      <td>/beyonce/11.html</td>\n",
       "      <td>BEYONCÉ [Platinum Edition]</td>\n",
       "      <td>If I ain't got nothing I got you If I ain't go...</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>if i ain t get nothing i get you if i ain t ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6 Inch (Feat. The Weeknd)</td>\n",
       "      <td>/beyonce/6-inch-feat-the-weeknd.html</td>\n",
       "      <td>LEMONADE</td>\n",
       "      <td>Six inch heels She walked in the club like nob...</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>six inch heels she walk in the club like nobod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Nome da Música  \\\n",
       "0                            '03 Bonnie & Clyde   \n",
       "1  ***Flawless (Feat. Chimamanda Ngozi Adichie)   \n",
       "2               ***Flawless (Feat. Nicki Minaj)   \n",
       "3                                           1+1   \n",
       "4                     6 Inch (Feat. The Weeknd)   \n",
       "\n",
       "                                                link  \\\n",
       "0                      /beyonce/03-bonnie-clyde.html   \n",
       "1  /beyonce/flawless-feat-chimamanda-ngozi-adichi...   \n",
       "2            /beyonce/flawless-feat-nicki-minaj.html   \n",
       "3                                   /beyonce/11.html   \n",
       "4               /beyonce/6-inch-feat-the-weeknd.html   \n",
       "\n",
       "                                               album  \\\n",
       "0  I Am... Yours: An Intimate Performance at Wynn...   \n",
       "1                                            BEYONCÉ   \n",
       "2                         BEYONCÉ [Platinum Edition]   \n",
       "3                         BEYONCÉ [Platinum Edition]   \n",
       "4                                           LEMONADE   \n",
       "\n",
       "                                               letra  artista  \\\n",
       "0  Jay-z Uh-uh-uh You ready b? Let's go get 'em. ...  Beyoncé   \n",
       "1  Your challengers are a young group from Housto...  Beyoncé   \n",
       "2  Dum-da-de-da Do, do, do, do, do, do (Coming do...  Beyoncé   \n",
       "3  If I ain't got nothing I got you If I ain't go...  Beyoncé   \n",
       "4  Six inch heels She walked in the club like nob...  Beyoncé   \n",
       "\n",
       "                                         Texto Limpo  \n",
       "0  jay z uh uh uh you ready b let s go get look y...  \n",
       "1  your challengers are young group from houston ...  \n",
       "2  dum come down drip candy on the ground it stay...  \n",
       "3  if i ain t get nothing i get you if i ain t ge...  \n",
       "4  six inch heels she walk in the club like nobod...  "
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() # vamos ver como ficou?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o96vg7AZPW61"
   },
   "source": [
    "## Feature Extraction\n",
    "Antes de treinar o nosso modelo, precisamos organizar os nossos documentos em features que o computador consegue entender, assim, vamos precisamos transformar o nosso texto em algum tipo de representação numérica. Para isso, vamos usar o Bag of Words. \n",
    "\n",
    "### Bag of Words \n",
    "**O que é o Bag of Words?:** BoW é uma forma de representação de texto que descreve a ocorrência de palavras em um documento. Para o BoW a ordem não importa, essa forma de representação só se importa se as palavras conhecidas ocorrem ou não no documento (literalmente um \"saco\" de palavras). \n",
    "\n",
    "Para implementarmos o Bag of Words, precisamos de três coisas: \n",
    "1. Um vocabulário com as palavras conhecidas\n",
    "2. A ocorrência dessas palavras\n",
    "3. Formar vetores a partir dos documentos \n",
    "\n",
    "**Exemplo**\n",
    "\n",
    "\"to the left to the left everything you own in the box to the left\"\n",
    "\n",
    "1. Construir o vocabulário\n",
    "\n",
    "    [\"to\", \"the\", \"left\", \"everything\", \"you\", \"own\", \"in\", \"box\"]\n",
    "    \n",
    "\n",
    "2. Ocorrência das palavras\n",
    "\n",
    "    {\"to\": 3, \"the\": 3, \"left\":3, \"everything\":1, \"you\":1, \"own\":1, \"in\":1, \"box\":1}\n",
    "\n",
    "\n",
    "3. Vetores\n",
    "\n",
    "    Considerando que o nosso documento fosse: \"to the left to the left\"\n",
    "\n",
    "    Usando o vocabulário que construímos antes, o nosso vetor seria: \n",
    "\n",
    "    [2, 2, 2, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Ggonk5WPW64"
   },
   "source": [
    "### Count Vectorizer \n",
    "Felizmente, temos o CountVectorizer! Com ele, conseguimos implementar todos os passos acima de uma maneira bem simples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OoiGvcEYPW67"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Bag of words\n",
    "count_vectorizer = CountVectorizer()\n",
    "X = count_vectorizer.fit_transform(df['Texto Limpo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpwp7cETPW7A"
   },
   "source": [
    "Olhando o nosso vocabulário: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vrncvpr2PW7A",
    "outputId": "b52bb5dc-387c-4dea-bc4e-57ed41d7300c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aaaaaah',\n",
       " 'aaah',\n",
       " 'aah',\n",
       " 'aahhhh',\n",
       " 'aaron',\n",
       " 'abandon',\n",
       " 'abanenkani',\n",
       " 'abaziyo',\n",
       " 'abit',\n",
       " 'abita',\n",
       " 'able',\n",
       " 'aboard',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abrasive',\n",
       " 'absolutely',\n",
       " 'abstain',\n",
       " 'abu',\n",
       " 'abunch',\n",
       " 'abuse',\n",
       " 'acabado',\n",
       " 'acabo',\n",
       " 'acabó',\n",
       " 'acaso',\n",
       " 'accent',\n",
       " 'accept',\n",
       " 'accepte',\n",
       " 'acceptin',\n",
       " 'access',\n",
       " 'accidentally',\n",
       " 'accomodation',\n",
       " 'accomplishments',\n",
       " 'account',\n",
       " 'accountant',\n",
       " 'accule',\n",
       " 'accusations',\n",
       " 'ace',\n",
       " 'ache',\n",
       " 'achetant',\n",
       " 'achieve',\n",
       " 'achètera',\n",
       " 'acompañarme',\n",
       " 'across',\n",
       " 'act',\n",
       " 'actin',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'activité',\n",
       " 'actor',\n",
       " 'actress',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'acuerdo',\n",
       " 'ad',\n",
       " 'add',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'addictive',\n",
       " 'address',\n",
       " 'adichie',\n",
       " 'adicto',\n",
       " 'adiós',\n",
       " 'adjust',\n",
       " 'adlibs',\n",
       " 'admire',\n",
       " 'admit',\n",
       " 'admittin',\n",
       " 'adolescent',\n",
       " 'adonde',\n",
       " 'adoration',\n",
       " 'adore',\n",
       " 'adorent',\n",
       " 'adrenaline',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advantage',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affectin',\n",
       " 'affection',\n",
       " 'affectionate',\n",
       " 'affliction',\n",
       " 'afford',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'afros',\n",
       " 'aftaparty',\n",
       " 'after',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'again',\n",
       " 'against',\n",
       " 'age',\n",
       " 'agent',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agressive',\n",
       " 'aguanto',\n",
       " 'aguilera',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'aham',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'ahhhh',\n",
       " 'ahhhhhhh',\n",
       " 'ahn',\n",
       " 'ahold',\n",
       " 'ahora',\n",
       " 'ahuh',\n",
       " 'ahí',\n",
       " 'ai',\n",
       " 'aim',\n",
       " 'aimes',\n",
       " 'ain',\n",
       " 'air',\n",
       " 'airplane',\n",
       " 'airplanes',\n",
       " 'airs',\n",
       " 'aisle',\n",
       " 'akhenaton',\n",
       " 'akon',\n",
       " 'al',\n",
       " 'alabama',\n",
       " 'alabies',\n",
       " 'alarm',\n",
       " 'alarms',\n",
       " 'albino',\n",
       " 'album',\n",
       " 'alcohol',\n",
       " 'alejandro',\n",
       " 'alert',\n",
       " 'algebra',\n",
       " 'alguien',\n",
       " 'ali',\n",
       " 'alicia',\n",
       " 'alight',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'alla',\n",
       " 'allan',\n",
       " 'allegiance',\n",
       " 'allergic',\n",
       " 'allergies',\n",
       " 'alley',\n",
       " 'allez',\n",
       " 'alligators',\n",
       " 'allow',\n",
       " 'alma',\n",
       " 'almighty',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'alot',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alt',\n",
       " 'alter',\n",
       " 'altercation',\n",
       " 'although',\n",
       " 'alumna',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amagwali',\n",
       " 'amanda',\n",
       " 'amandla',\n",
       " 'amante',\n",
       " 'amaphi',\n",
       " 'amaqhawe',\n",
       " 'amar',\n",
       " 'amarte',\n",
       " 'amaze',\n",
       " 'amazing',\n",
       " 'ambas',\n",
       " 'ambition',\n",
       " 'ambulance',\n",
       " 'amen',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amigos',\n",
       " 'amiss',\n",
       " 'amistad',\n",
       " 'amo',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amor',\n",
       " 'amore',\n",
       " 'amount',\n",
       " 'amour',\n",
       " 'amp',\n",
       " 'amplify',\n",
       " 'amélioration',\n",
       " 'an',\n",
       " 'analogy',\n",
       " 'ancestors',\n",
       " 'and',\n",
       " 'anda',\n",
       " 'andasians',\n",
       " 'andoyiki',\n",
       " 'andre',\n",
       " 'ange',\n",
       " 'angel',\n",
       " 'angels',\n",
       " 'ankh',\n",
       " 'ankles',\n",
       " 'annonce',\n",
       " 'another',\n",
       " 'ans',\n",
       " 'answer',\n",
       " 'answers',\n",
       " 'ant',\n",
       " 'anticipate',\n",
       " 'anticipation',\n",
       " 'antidote',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'aorte',\n",
       " 'apart',\n",
       " 'apollo',\n",
       " 'apologize',\n",
       " 'apology',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appealing',\n",
       " 'appear',\n",
       " 'appease',\n",
       " 'appetite',\n",
       " 'applaud',\n",
       " 'applause',\n",
       " 'apply',\n",
       " 'appreciate',\n",
       " 'approach',\n",
       " 'appropriate',\n",
       " 'approve',\n",
       " 'après',\n",
       " 'aps',\n",
       " 'aquarian',\n",
       " 'aquarius',\n",
       " 'aqueducts',\n",
       " 'aquella',\n",
       " 'aquí',\n",
       " 'ardeur',\n",
       " 'ardor',\n",
       " 'are',\n",
       " 'area',\n",
       " 'aren',\n",
       " 'arena',\n",
       " 'arenas',\n",
       " 'argu',\n",
       " 'argue',\n",
       " 'argueing',\n",
       " 'arguement',\n",
       " 'aries',\n",
       " 'aristocracy',\n",
       " 'arm',\n",
       " 'armada',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'around',\n",
       " 'arrest',\n",
       " 'arrive',\n",
       " 'arrogance',\n",
       " 'arrogant',\n",
       " 'arrosent',\n",
       " 'ars',\n",
       " 'art',\n",
       " 'arthur',\n",
       " 'article',\n",
       " 'articulate',\n",
       " 'artist',\n",
       " 'asco',\n",
       " 'ashamed',\n",
       " 'ashanti',\n",
       " 'ashes',\n",
       " 'ashley',\n",
       " 'ashtray',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'askin',\n",
       " 'asking',\n",
       " 'asleep',\n",
       " 'asphalt',\n",
       " 'aspiration',\n",
       " 'aspire',\n",
       " 'ass',\n",
       " 'assassin',\n",
       " 'asses',\n",
       " 'asshole',\n",
       " 'assume',\n",
       " 'assure',\n",
       " 'astaire',\n",
       " 'aswe',\n",
       " 'así',\n",
       " 'at',\n",
       " 'atch',\n",
       " 'atención',\n",
       " 'atl',\n",
       " 'atlanta',\n",
       " 'atmosphere',\n",
       " 'atomic',\n",
       " 'attach',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'attendance',\n",
       " 'attendant',\n",
       " 'attention',\n",
       " 'attire',\n",
       " 'attirent',\n",
       " 'attitude',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'au',\n",
       " 'audacity',\n",
       " 'audemars',\n",
       " 'audibles',\n",
       " 'auditionin',\n",
       " 'audmars',\n",
       " 'audubon',\n",
       " 'aunque',\n",
       " 'aura',\n",
       " 'austin',\n",
       " 'aut',\n",
       " 'automne',\n",
       " 'autre',\n",
       " 'aux',\n",
       " 'avail',\n",
       " 'avant',\n",
       " 'ave',\n",
       " 'avenue',\n",
       " 'average',\n",
       " 'aveugle',\n",
       " 'avi',\n",
       " 'aviator',\n",
       " 'avoid',\n",
       " 'aw',\n",
       " 'awahlangani',\n",
       " 'awake',\n",
       " 'awaken',\n",
       " 'award',\n",
       " 'aware',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'awful',\n",
       " 'awfully',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'axel',\n",
       " 'ay',\n",
       " 'aye',\n",
       " 'ayo',\n",
       " 'ayy',\n",
       " 'az',\n",
       " 'azealia',\n",
       " 'azul',\n",
       " 'ba',\n",
       " 'babe',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'babymomma',\n",
       " 'bach',\n",
       " 'back',\n",
       " 'background',\n",
       " 'backless',\n",
       " 'backs',\n",
       " 'backseat',\n",
       " 'backslash',\n",
       " 'backstreet',\n",
       " 'backwards',\n",
       " 'bad',\n",
       " 'badda',\n",
       " 'badder',\n",
       " 'baddest',\n",
       " 'baddiebey',\n",
       " 'badibadee',\n",
       " 'badonkadonk',\n",
       " 'bae',\n",
       " 'bag',\n",
       " 'baggage',\n",
       " 'baggy',\n",
       " 'bags',\n",
       " 'bahamas',\n",
       " 'bail',\n",
       " 'baila',\n",
       " 'bailando',\n",
       " 'bailey',\n",
       " 'baileys',\n",
       " 'baiser',\n",
       " 'bajan',\n",
       " 'bajans',\n",
       " 'bajo',\n",
       " 'bajun',\n",
       " 'bake',\n",
       " 'bal',\n",
       " 'balance',\n",
       " 'bald',\n",
       " 'ball',\n",
       " 'ballers',\n",
       " 'ballet',\n",
       " 'ballin',\n",
       " 'balloon',\n",
       " 'balls',\n",
       " 'bally',\n",
       " 'bama',\n",
       " 'ban',\n",
       " 'band',\n",
       " 'bandanas',\n",
       " 'bands',\n",
       " 'bang',\n",
       " 'bangin',\n",
       " 'bangkok',\n",
       " 'bank',\n",
       " 'bankhead',\n",
       " 'banks',\n",
       " 'banner',\n",
       " 'baobab',\n",
       " 'baptize',\n",
       " 'bar',\n",
       " 'barbados',\n",
       " 'barbarian',\n",
       " 'barbeque',\n",
       " 'barber',\n",
       " 'barbie',\n",
       " 'bare',\n",
       " 'barefoot',\n",
       " 'barely',\n",
       " 'bargain',\n",
       " 'bark',\n",
       " 'barneys',\n",
       " 'barretta',\n",
       " 'barricades',\n",
       " 'bars',\n",
       " 'bas',\n",
       " 'base',\n",
       " 'basement',\n",
       " 'bask',\n",
       " 'basquiat',\n",
       " 'bass',\n",
       " 'bassline',\n",
       " 'basta',\n",
       " 'bastards',\n",
       " 'bath',\n",
       " 'bathe',\n",
       " 'bathroom',\n",
       " 'battle',\n",
       " 'battles',\n",
       " 'bawl',\n",
       " 'bay',\n",
       " 'bayou',\n",
       " 'bb',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'beaches',\n",
       " 'beads',\n",
       " 'beams',\n",
       " 'bear',\n",
       " 'beast',\n",
       " 'beaste',\n",
       " 'beat',\n",
       " 'beats',\n",
       " 'beaucoup',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'beauty',\n",
       " 'becah',\n",
       " 'because',\n",
       " 'becky',\n",
       " 'become',\n",
       " 'bed',\n",
       " 'bedingfield',\n",
       " 'bedpost',\n",
       " 'bedroom',\n",
       " 'bedtime',\n",
       " 'bee',\n",
       " 'beeeeze',\n",
       " 'beef',\n",
       " 'beefs',\n",
       " 'been',\n",
       " 'beer',\n",
       " 'bees',\n",
       " 'beeze',\n",
       " 'befall',\n",
       " 'before',\n",
       " 'beg',\n",
       " 'beggers',\n",
       " 'beggin',\n",
       " 'beggining',\n",
       " 'begin',\n",
       " 'beginnig',\n",
       " 'beginnin',\n",
       " 'beginning',\n",
       " 'behave',\n",
       " 'behind',\n",
       " 'beholder',\n",
       " 'bein',\n",
       " 'being',\n",
       " 'beings',\n",
       " 'belief',\n",
       " 'beliefs',\n",
       " 'believe',\n",
       " 'believer',\n",
       " 'believin',\n",
       " 'bell',\n",
       " 'bello',\n",
       " 'bells',\n",
       " 'belong',\n",
       " 'belt',\n",
       " 'belushi',\n",
       " 'bendin',\n",
       " 'beneath',\n",
       " 'benjis',\n",
       " 'benoit',\n",
       " 'bent',\n",
       " 'bentley',\n",
       " 'bentz',\n",
       " 'benz',\n",
       " 'berret',\n",
       " 'berry',\n",
       " 'bes',\n",
       " 'besar',\n",
       " 'beside',\n",
       " 'best',\n",
       " 'besta',\n",
       " 'bestest',\n",
       " 'bestfriends',\n",
       " 'besándote',\n",
       " 'bet',\n",
       " 'betcha',\n",
       " 'bethlehem',\n",
       " 'betray',\n",
       " 'betta',\n",
       " 'bette',\n",
       " 'better',\n",
       " 'between',\n",
       " 'bey',\n",
       " 'beyonce',\n",
       " 'beyoncè',\n",
       " 'beyoncé',\n",
       " 'beyond',\n",
       " 'bezel',\n",
       " 'bi',\n",
       " 'bible',\n",
       " 'biddle',\n",
       " 'bien',\n",
       " 'bienvenue',\n",
       " 'bifteck',\n",
       " 'bifton',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'biggie',\n",
       " 'bike',\n",
       " 'bikini',\n",
       " 'bill',\n",
       " 'billi',\n",
       " 'billion',\n",
       " 'bills',\n",
       " 'bind',\n",
       " 'binds',\n",
       " 'birds',\n",
       " 'birkin',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'birthright',\n",
       " 'bisbal',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bitches',\n",
       " 'bite',\n",
       " 'bitter',\n",
       " 'bittersweet',\n",
       " 'bitty',\n",
       " 'black',\n",
       " 'blackbirds',\n",
       " 'blackjack',\n",
       " 'blackness',\n",
       " 'blacks',\n",
       " 'bladder',\n",
       " 'blade',\n",
       " 'blague',\n",
       " 'blah',\n",
       " 'blahnik',\n",
       " 'blaine',\n",
       " 'blake',\n",
       " 'blame',\n",
       " 'blast',\n",
       " 'blaze',\n",
       " 'blazer',\n",
       " 'blazin',\n",
       " 'blazing',\n",
       " 'ble',\n",
       " 'bleach',\n",
       " 'bleed',\n",
       " 'bleek',\n",
       " 'blend',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'blessin',\n",
       " 'blessing',\n",
       " 'blessings',\n",
       " 'blige',\n",
       " 'blind',\n",
       " 'blindfold',\n",
       " 'blindly',\n",
       " 'blindés',\n",
       " 'bliss',\n",
       " 'blisters',\n",
       " 'blizzards',\n",
       " 'block',\n",
       " 'blocks',\n",
       " 'bloggers',\n",
       " 'blogs',\n",
       " 'blonder',\n",
       " 'blood',\n",
       " 'bloodline',\n",
       " 'bloom',\n",
       " 'bloomberg',\n",
       " 'blouse',\n",
       " 'blow',\n",
       " 'blowin',\n",
       " 'blubland',\n",
       " 'blue',\n",
       " 'bluebirds',\n",
       " 'blues',\n",
       " 'bluff',\n",
       " 'blunt',\n",
       " 'blush',\n",
       " 'bmw',\n",
       " 'bo',\n",
       " 'board',\n",
       " 'boards',\n",
       " 'boat',\n",
       " 'bobbly',\n",
       " 'bobby',\n",
       " 'bodied',\n",
       " 'bodies',\n",
       " 'body',\n",
       " 'boi',\n",
       " 'boilin',\n",
       " 'bold',\n",
       " 'boma',\n",
       " 'bomaye',\n",
       " 'bomb',\n",
       " 'bombs',\n",
       " 'bon',\n",
       " 'bonds',\n",
       " 'bone',\n",
       " 'boner',\n",
       " 'bones',\n",
       " 'boney',\n",
       " 'bonheur',\n",
       " 'bonjour',\n",
       " 'bonnie',\n",
       " 'bonny',\n",
       " 'bono',\n",
       " 'bony',\n",
       " 'boo',\n",
       " 'boogie',\n",
       " 'boojy',\n",
       " 'book',\n",
       " 'books',\n",
       " 'boom',\n",
       " 'boomers',\n",
       " 'boost',\n",
       " 'boots',\n",
       " 'booty',\n",
       " 'bootyb',\n",
       " 'bootylicious',\n",
       " 'boppers',\n",
       " 'border',\n",
       " 'borders',\n",
       " 'bore',\n",
       " 'borrow',\n",
       " 'boss',\n",
       " 'bossy',\n",
       " 'both',\n",
       " 'bother',\n",
       " 'bottle',\n",
       " 'bottles',\n",
       " 'bottom',\n",
       " 'bottomless',\n",
       " 'boucher',\n",
       " 'boudin',\n",
       " 'boudoir',\n",
       " 'bouffes',\n",
       " 'bouger',\n",
       " 'boulevard',\n",
       " 'bounce',\n",
       " 'bouncin',\n",
       " 'bouncing',\n",
       " 'boundless',\n",
       " 'bount',\n",
       " 'bouquet',\n",
       " 'bout',\n",
       " 'bow',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boyboy',\n",
       " 'boyfirend',\n",
       " 'boyfriend',\n",
       " 'boys',\n",
       " 'boyy',\n",
       " 'boyyyy',\n",
       " 'bp',\n",
       " 'bra',\n",
       " 'brace',\n",
       " 'braggin',\n",
       " 'braid',\n",
       " 'braids',\n",
       " 'brain',\n",
       " 'brains',\n",
       " 'brand',\n",
       " 'brander',\n",
       " 'brando',\n",
       " 'brap',\n",
       " 'brat',\n",
       " 'brave',\n",
       " 'brawls',\n",
       " 'brazil',\n",
       " 'brazilian',\n",
       " 'bre',\n",
       " 'bread',\n",
       " 'break',\n",
       " 'breakdown',\n",
       " 'breakfast',\n",
       " 'breakin',\n",
       " 'breaking',\n",
       " 'breaks',\n",
       " 'breakups',\n",
       " 'breate',\n",
       " 'breath',\n",
       " 'breathe',\n",
       " 'breathin',\n",
       " 'breathing',\n",
       " 'breezin',\n",
       " 'brick',\n",
       " 'bricks',\n",
       " 'bridge',\n",
       " 'briefcase',\n",
       " 'bright',\n",
       " 'brighter',\n",
       " 'brille',\n",
       " 'brilliant',\n",
       " 'brillo',\n",
       " 'brim',\n",
       " 'bring',\n",
       " 'brink',\n",
       " 'britney',\n",
       " 'broad',\n",
       " 'broads',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'bronche',\n",
       " 'bronx',\n",
       " 'brooklyn',\n",
       " 'brotha',\n",
       " 'brothas',\n",
       " 'brother',\n",
       " 'brown',\n",
       " 'browsing',\n",
       " 'bruce',\n",
       " 'bruh',\n",
       " 'bruk',\n",
       " 'bruno',\n",
       " 'brunormoliveira',\n",
       " 'brush',\n",
       " 'brushin',\n",
       " 'brutality',\n",
       " 'bub',\n",
       " 'bubble',\n",
       " 'bubbles',\n",
       " 'buck',\n",
       " 'buckle',\n",
       " 'buddha',\n",
       " 'buddy',\n",
       " 'buds',\n",
       " 'bue',\n",
       " 'bug',\n",
       " 'bugatti',\n",
       " 'build',\n",
       " 'building',\n",
       " 'buildings',\n",
       " 'bullet',\n",
       " 'bulletproof',\n",
       " 'bulls',\n",
       " 'bully',\n",
       " 'bum',\n",
       " 'bumm',\n",
       " 'bump',\n",
       " 'bumpin',\n",
       " 'bun',\n",
       " 'bunny',\n",
       " 'burberry',\n",
       " 'burbs',\n",
       " 'burden',\n",
       " 'bureaux',\n",
       " 'burn',\n",
       " 'burnin',\n",
       " 'burning',\n",
       " 'burns',\n",
       " 'burress',\n",
       " 'burst',\n",
       " 'bury',\n",
       " 'bus',\n",
       " 'buscar',\n",
       " 'buscaré',\n",
       " 'bush',\n",
       " 'business',\n",
       " 'buss',\n",
       " 'bust',\n",
       " 'busta',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'butt',\n",
       " 'butta',\n",
       " 'butter',\n",
       " 'butterflies',\n",
       " 'button',\n",
       " 'buttons',\n",
       " 'buy',\n",
       " 'bvalgari',\n",
       " 'bwoy',\n",
       " 'by',\n",
       " 'bye',\n",
       " 'byeee',\n",
       " 'ca',\n",
       " 'caan',\n",
       " 'cab',\n",
       " 'cabeza',\n",
       " 'cabinets',\n",
       " 'cable',\n",
       " 'cada',\n",
       " 'caddy',\n",
       " 'cadillac',\n",
       " 'caen',\n",
       " 'caer',\n",
       " 'caesar',\n",
       " 'cage',\n",
       " 'cah',\n",
       " 'cain',\n",
       " 'cake',\n",
       " 'california',\n",
       " 'call',\n",
       " 'callar',\n",
       " 'callin',\n",
       " 'callo',\n",
       " 'calls',\n",
       " 'calm',\n",
       " 'calma',\n",
       " 'calmin',\n",
       " 'calvin',\n",
       " 'calypso',\n",
       " 'cameo',\n",
       " 'camera',\n",
       " 'cameras',\n",
       " 'camino',\n",
       " 'camo',\n",
       " 'campbell',\n",
       " 'camry',\n",
       " 'can',\n",
       " 'cancel',\n",
       " 'cancer',\n",
       " 'canción',\n",
       " 'candelabra',\n",
       " 'candidate',\n",
       " 'candle',\n",
       " 'candles',\n",
       " 'candy',\n",
       " 'cane',\n",
       " 'caniveau',\n",
       " 'cannonball',\n",
       " 'canse',\n",
       " 'cap',\n",
       " 'capacity',\n",
       " 'capaz',\n",
       " 'cape',\n",
       " 'capital',\n",
       " 'capricorn',\n",
       " 'capricorns',\n",
       " 'captain',\n",
       " 'captivate',\n",
       " 'capture',\n",
       " 'car',\n",
       " 'cara',\n",
       " 'carbon',\n",
       " 'card',\n",
       " 'cardiac',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'career',\n",
       " 'careful',\n",
       " 'carefully',\n",
       " 'cares',\n",
       " 'carey',\n",
       " 'carline',\n",
       " 'carlo',\n",
       " 'carlos',\n",
       " 'carmen',\n",
       " 'carriage',\n",
       " 'carribean',\n",
       " 'carrie',\n",
       " 'carry',\n",
       " 'cars',\n",
       " 'carter',\n",
       " 'carters',\n",
       " 'cartier',\n",
       " 'carve',\n",
       " 'casanova',\n",
       " 'cascade',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'casino',\n",
       " 'caskets',\n",
       " 'cast',\n",
       " 'castle',\n",
       " 'castles',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'catchin',\n",
       " 'catching',\n",
       " 'category',\n",
       " 'catfishes',\n",
       " 'cats',\n",
       " 'catwalk',\n",
       " 'cause',\n",
       " 'cavalier',\n",
       " 'cave',\n",
       " 'cavities',\n",
       " 'cayendo',\n",
       " 'cayó',\n",
       " 'caímo',\n",
       " 'caímos',\n",
       " 'caïds',\n",
       " 'cd',\n",
       " 'ce',\n",
       " 'cee',\n",
       " 'ceed',\n",
       " 'ceiling',\n",
       " 'ceilings',\n",
       " 'celebration',\n",
       " 'celebrity',\n",
       " 'celie',\n",
       " 'cell',\n",
       " 'cellphone',\n",
       " 'center',\n",
       " 'centigrade',\n",
       " 'central',\n",
       " 'century',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'certified',\n",
       " 'certify',\n",
       " 'ces',\n",
       " 'ceux',\n",
       " 'cha',\n",
       " 'chain',\n",
       " 'chains',\n",
       " 'chair',\n",
       " 'chaka',\n",
       " 'challenge',\n",
       " 'challenger',\n",
       " 'challengers',\n",
       " 'cham',\n",
       " 'champa',\n",
       " 'champagne',\n",
       " 'champion',\n",
       " 'champions',\n",
       " 'chance',\n",
       " 'chances',\n",
       " 'chandeliers',\n",
       " 'chanel',\n",
       " 'change',\n",
       " 'changent',\n",
       " 'changes',\n",
       " 'channel',\n",
       " 'chaque',\n",
       " 'character',\n",
       " 'characters',\n",
       " 'chardonnay',\n",
       " 'charge',\n",
       " 'chariot',\n",
       " 'charlie',\n",
       " 'charm',\n",
       " 'charter',\n",
       " 'chase',\n",
       " 'chaser',\n",
       " 'chasin',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names() #Todas as palavras do nosso vocabulário "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsoJ7CQkPW7G",
    "outputId": "8fc4c0e3-ee08-4c2c-ed37-c7e7f7d23898"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3543"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.vocabulary_.get('love')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mTT5x5WLPW7M"
   },
   "source": [
    "Exemplo da nossa matriz termo-documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XpWLbwtIPW7N",
    "outputId": "4c313714-dbac-4f2d-99b5-2893b8ab92ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaaah</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aah</th>\n",
       "      <th>aahhhh</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abanenkani</th>\n",
       "      <th>abaziyo</th>\n",
       "      <th>abit</th>\n",
       "      <th>...</th>\n",
       "      <th>égaux</th>\n",
       "      <th>élever</th>\n",
       "      <th>élu</th>\n",
       "      <th>éléverons</th>\n",
       "      <th>état</th>\n",
       "      <th>été</th>\n",
       "      <th>évite</th>\n",
       "      <th>évoque</th>\n",
       "      <th>êt</th>\n",
       "      <th>única</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6904 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaaaaah  aaah  aah  aahhhh  aaron  abandon  abanenkani  abaziyo  abit  \\\n",
       "0   0        0     0    0       0      0        0           0        0     0   \n",
       "1   0        0     0    0       0      0        0           0        0     0   \n",
       "2   0        0     0    0       0      0        0           0        0     0   \n",
       "3   0        0     0    0       0      0        0           0        0     0   \n",
       "4   0        0     0    0       0      0        0           0        0     0   \n",
       "\n",
       "   ...  égaux  élever  élu  éléverons  état  été  évite  évoque  êt  única  \n",
       "0  ...      0       0    0          0     0    0      0       0   0      0  \n",
       "1  ...      0       0    0          0     0    0      0       0   0      0  \n",
       "2  ...      0       0    0          0     0    0      0       0   0      0  \n",
       "3  ...      0       0    0          0     0    0      0       0   0      0  \n",
       "4  ...      0       0    0          0     0    0      0       0   0      0  \n",
       "\n",
       "[5 rows x 6904 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv = pd.DataFrame(X.toarray(), columns = count_vectorizer.get_feature_names())\n",
    "df_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OQasqKkLPW7g"
   },
   "source": [
    "No dataframe acima, cada uma das colunas representa uma das palavras do nosso vocabulário, e cada linha, um dos nossos documentos, ou seja, uma das nossas músicas. \n",
    "\n",
    "Como são muitas colunas, vamos inspecionar apenas dez delas:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QApZuOgaPW7h",
    "outputId": "c27299a6-df92-46da-9bef-8e3223e3362c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abita</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>about</th>\n",
       "      <th>above</th>\n",
       "      <th>abrasive</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abstain</th>\n",
       "      <th>abu</th>\n",
       "      <th>abunch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>508 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     abita  able  aboard  about  above  abrasive  absolutely  abstain  abu  \\\n",
       "0        0     0       0      1      0         0           0        0    0   \n",
       "1        0     0       0      0      0         0           0        0    0   \n",
       "2        0     0       0      1      0         0           0        0    0   \n",
       "3        0     0       0      3      0         0           0        0    0   \n",
       "4        0     0       0      0      0         0           0        0    0   \n",
       "..     ...   ...     ...    ...    ...       ...         ...      ...  ...   \n",
       "503      0     0       0      1      0         0           0        0    0   \n",
       "504      0     0       0     14      0         0           0        0    0   \n",
       "505      0     0       0      0      0         0           0        0    0   \n",
       "506      0     0       0      0      0         0           0        0    0   \n",
       "507      0     0       0      2      0         0           0        0    0   \n",
       "\n",
       "     abunch  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "..      ...  \n",
       "503       0  \n",
       "504       0  \n",
       "505       0  \n",
       "506       0  \n",
       "507       0  \n",
       "\n",
       "[508 rows x 10 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv.iloc[:, 10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vRM4UHlsPW7r"
   },
   "source": [
    "Podemos observar que a palavra \"about\" aparece 14 vezes no documento 504. Vamos investigar isso: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2BeViEDsPW7w",
    "outputId": "a783697a-1f8f-43ea-a38e-f4fcd2568aef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you the one that i dream about all day you the one that i think about always you are the one so i make sure i behave my love is your love your love is my love baby i love you i need you here with all the time ime baby we mean to be you get smile all the time ime cause you know how to give that you know how to pull back when i go runnin runnin tryin to get away from love ya you know how to love hard i win t lie i m fall hard yep i m fall ya but there s nothin wrong with that you the one that i dream about all day you the one that i think about always you are the one so i make sure i behave my love is your love your love is my love you the one that i dream about all day you the one that i think about always you are the one so i make sure i behave my love is your love your love is mine baby come take now hold now make come alive i have you get the sweetest touch i m so happy you come in my life life cause you know how to give that you know how to pull back when i go runnin runnin tryin to get away from love ya you know how to love hard i win t lie i m fall hard yep i m fall ya but there s nothin wrong with that you the one that i dream about all day you the one that i think about always you are the one so i make sure i behave my love is your love your love is my love you the one that i dream about all day you the one that i think about always you are the one so i make sure i behave my love is your love your love is mine yes i m kinda crazy that s what happen baby when you put it down you shouldn t ve give it to good like that shouldn t ve hit it like that had yellin like that didn t know you would ve had come back you the one that i m feel you the one that i m love ain t other ni like you there s just one one one one baby just one one one one i bet you wanna know you the one that i dream about all day you the one that i think about always you are the one so i make sure i behave my love is your love your love is my love you the one that i dream about all day you the one that i think about always you are the one so i make sure i behave my love is your love your love is mine'"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[504, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NdoQSHtqPW7-"
   },
   "source": [
    "De fato, essa música apr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uGmN5MILPW8A"
   },
   "source": [
    "## Separando em Treino e Teste\n",
    "- explicar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wieWN86PW8C"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X.toarray()\n",
    "y = df['artista']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V03QPr0QPW8H"
   },
   "source": [
    "## Naive Bayes\n",
    "- Explicar modelo/aplicação "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJKMuSYLPW8J"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Criando o Modelo Naive Bayes \n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "#.......Treinando o Modelo.......\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "#Fazendo as previsões\n",
    "naive_bayes_pred = naive_bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJNecXsIPW8f"
   },
   "source": [
    "## Métricas \n",
    "Após estarmos com nosso modelo de classificação pronto, devemos avaliá-lo e, para isso, utilizamos as métricas de classificação. \n",
    "### Matriz de confusão\n",
    "Primeiro, quando estamos lidando com um modelo cuja target é categórica (como no nosso caso, em que as músicas pertencem ou a Beyoncé ou a Rihanna), podemos utilizar uma matriz de confusão para analisarmos melhor onde o nosso modelo está acertando e onde ele está errando. Ela apresenta o seguinte formato:\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Fabio_Araujo_Da_Silva/publication/323369673/figure/fig5/AS:597319787479040@1519423543307/Figura-13-Exemplo-de-uma-matriz-de-confusao.png\" alt=\"Exemplo de uma matriz de confusão\"/></a>\n",
    "\n",
    "Na vertical, estão indicados os valores previstos pelo modelo e, na horizontal, os valores reais. Para cada elemento da matriz, temos dois valores associados: o previsto e o real. Se esse valores coincidirem, tem-se uma previsão correta/verdadeira (por exemplo, verdadeiros positivos e verdadeiros negativos, que estão em verde na imagem). Caso contrário, tem-se um erro cometido pelo modelo (como ocorre nos quadrados vermelhos da imagem acima). \n",
    "\n",
    "### Acurácia\n",
    "A acurácia é, basicamente, uma métrica que indica a relação entre quanto o seu modelo acertou do quanto ele avaliou. Considerando a matriz de confusão mostrada, a acurácia seria igual à soma dos verdadeiros positivos com os verdadeiros negativos dividida pelo total (soma dos verdadeiros e falsos positivos e negativos). A acurácia não é uma boa métrica a ser utilizada quando analisamos dados desbalanceados, porque pode acontecer de o modelo prever muito bem o evento mais usual e ser péssimo prevendo o evento raro. Assim, como trata-se de uma média simples de acertos pelo total, a grande quantidade de acertos na previsão do evento mais usual compensaria a baixa taxa de acerto do evento raro, resultando em uma acurácia alta que não reflete corretamente a qualidade de predição do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wi75e7PvPW8g",
    "outputId": "01433652-7bb5-44d7-ecb9-36128ee22e08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo 0.6764705882352942\n",
      "\n",
      "Matriz de confusão: \n",
      " [[39  9]\n",
      " [24 30]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "#Calculando a acurácia\n",
    "acc = accuracy_score(naive_bayes_pred, y_test)\n",
    "\n",
    "#Matriz de confusão \n",
    "cm = confusion_matrix(naive_bayes_pred, y_test)\n",
    "\n",
    "print(\"Acurácia do modelo\", acc)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6mo3KiPPW8w"
   },
   "source": [
    "## Avaliando as músicas\n",
    "- Tirei as músicas \"Drunk in Love\" (Beyoncé) e \"Diamonds\" (Rihanna) do dataset para testarmos na mão se o modelo consegue prever as cantoras corretamente: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FjPysbIzPW82",
    "outputId": "d02ed419-177e-44b5-b440-fcf16aa86c01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beyoncé']\n"
     ]
    }
   ],
   "source": [
    "frase_beyonce = [\"i ve been drinking i ve been drinking i get filthy when that liquor gets into i ve been thinking i ve been thinking why can t i keep my fingers off you baby i want you why can t i keep my fingers off you baby i want you cigars on ice cigars on ice feeling like an animal with these cameras all in my grill flashing lights flashing lights you got faded faded faded baby i want you can t keep your eyes off my fatty daddy i want you drunk in love drunk in love we be all night last thing i remember is our beautiful bodies grinding off in that club drunk in love we be all night love love we be all night love we be all night and everything alright complaints cause my body so fluorescent under these lights boy i m drinking walking in my l assemblage i m grubbing on the rope grubbing if you scared call that reverend boy i m drinking get my brain right i m on the cognac gangster wife new sheets he d swear that i like washed rags he wet up boy i m drinking i m sinking on the mic til my boy toys then i fill the tub up halfway then ride it with my surfboard surfboard surfboard graining on that wood graining graining on that wood i m swerving on that swerving swerving on that big body benz serving all this swerv surfing all of this good good drunk in love we be all night last thing i remember is our beautiful bodies grinding off in that club drunk in love we be all night love love we be all night love love\"] \n",
    "teste_b = count_vectorizer.transform(frase_beyonce)\n",
    "pred_b = naive_bayes.predict(teste_b)\n",
    "print(pred_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQ84io25PW9K",
    "outputId": "b50b3762-c8b4-453e-f393-64faa874c916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beyoncé']\n"
     ]
    }
   ],
   "source": [
    "frase_rihanna = [\"shine bright like diamond shine bright like diamond find light in the beautiful sea i choose to be happy you and i you and i we re like diamonds in the sky you re shooting star i see vision of ecstasy when you hold i m alive we re like diamonds in the sky i knew that we d become one right away oh right away at first sight i felt the energy of sun rays i saw the life inside your eyes so shine bright tonight you and i we re beautiful like diamonds in the sky eye to eye so alive we re beautiful like diamonds in the sky shine bright like diamond shine bright like diamond shine bright like diamond we re beautiful like diamonds in the sky shine bright like diamond shine bright like diamond shine bright like diamond we re beautiful like diamonds in the sky palms rise to the universe we moonshine and molly feel the warmth we ll never die we re like diamonds in the sky you re shooting star i see vision of ecstasy when you hold i m alive we re like diamonds in the sky at first sight i felt the energy of sun rays i saw the life inside your eyes so shine bright tonight you and i we re beautiful like diamonds in the sky eye to eye so alive we re beautiful like diamonds in the sky shine bright like diamond shine bright like diamond shine bright like diamond we re beautiful like diamonds in the sky shine bright like diamond shine bright like diamond shine bright like diamond we re beautiful like diamonds in the sky shine bright like diamond shine bright like diamond shine bright like diamond we re beautiful like diamonds in the sky shine bright like diamond shine bright like diamond shine bright like diamond so shine bright tonight you and i we re beautiful like diamonds in the sky eye to eye so alive we re beautiful like diamonds in the sky shine bright like diamond shine bright like diamond shine bright like diamond shine bright like diamond shine bright like diamond shine bright like diamond shine bright like diamond\"] \n",
    "teste_r = count_vectorizer.transform(frase_rihanna)\n",
    "pred_r = naive_bayes.predict(teste_r)\n",
    "print(pred_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UwSpOXN0PW9U"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "classificador.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

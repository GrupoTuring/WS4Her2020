{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJbFuG-Ugv02"
   },
   "source": [
    "# Classificador\n",
    "\n",
    "Neste Workshop, vamos aprender como criar um classificador que diz quais músicas são da Rihanna e quais são da Beyoncé utilizando aprendizado de máquina, mais especificamente *Aprendizado Supervisionado*, uma das áreas de Machine Learning.\n",
    "\n",
    "Então vamos lá!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Qqq3M_vPW2p"
   },
   "source": [
    "## Importando os dados\n",
    "\n",
    "O primeiro passo é importar seus dados, no caso nosso Dataframe (como chamamos a *tabela* que guarda as informações que usaremos). Fazemos isso com a biblioteca `pandas`.\n",
    "\n",
    "Com a função `read_csv` lemos nosso arquivo e guardamos ele na váriavel `df`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DC8V9WPPW2w"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dv9XVp4NPW25",
    "outputId": "db1785f4-8c19-4e92-d683-ad6ace15c439"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('beyonce_rihanna.csv', index_col=0)\n",
    "\n",
    "# vamos explorar nosso dataframe olhando apenas as primeiras linhas com a função abaixo:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a63DRicqPW3G"
   },
   "source": [
    "## Pré-processamentos\n",
    "\n",
    "Antes de partir para o aprendizado de máquina, precisamos preparar nosso texto. Fazemos isso porque, para a máquina, algumas palavras ou estruturas do nosso texto não importam e não fazem diferença. \n",
    "São muitos os métodos de pré-processamento, mas aqui vamos realizar apenas alguns: \n",
    "* Tokenização\n",
    "* Remover stopwords\n",
    "* Deixar todo o texto em minúsculo\n",
    "* Selecionar apenas letras com REGEX\n",
    "* Lemmatização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "poFLBHOcPW3J",
    "outputId": "a74b3c99-759e-4621-b88a-5a40b534dc40"
   },
   "outputs": [],
   "source": [
    "# Utilizando uma música como exemplo\n",
    "exemplo = df['letra'][10]\n",
    "exemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dY_Ku-a_PW3b"
   },
   "source": [
    "### Tokenização \n",
    "\n",
    "Uma parte importante no pré-processamento de um texto é a tokenização. Isto é, transformar elementos do seu texto em tokens, ou seja, strings dentro de uma lista  -  ou, se você não tiver conhecimento de python, transformar todas as palavras do texto em elementos individuais separados por aspas. \n",
    "Podemos tokenizar palavras com `word_tokenize`, essa função recebe o texto como argumento e retorna todas as palavras do texto em forma de tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vcXQSNTOPW3c"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgTkezQEPW3n",
    "outputId": "776a61d5-5363-4c0c-b4e4-a7691ad4b5ad"
   },
   "outputs": [],
   "source": [
    "#Tokenizando a primeira música\n",
    "tokens = word_tokenize(exemplo)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8iX6icQ4PW34"
   },
   "source": [
    "### Selecionando apenas as letras e deixando todas em minúsculas\n",
    "\n",
    "Para a máquina, pontuações não são necessárias, por isso um pré-processamento necessário é selecionar apenas as letras de um texto. \n",
    "\n",
    "Porém, **antes disso** precisamos deixar todas as letras em minúsculo, não somente porque isso facilita a aplicação do REGEX, mas também porque a máquina tende a interpretar palavras com letras maiúsculas e minúsculas como sendo diferentes. Por exemplo, Beyoncé e beyoncé podem ser interpretadas como palavras distintas. Então vamos deixar as letras minúsculas com a função `.lower`.\n",
    "\n",
    "Feito isso, podemos selecionar apenas as letras com REGEX, mais especificamente com a função `re.findall`, que, além de retornar apenas as letras, já tokeniza o texto para você! \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWfHh-yfPW38",
    "outputId": "e1653025-f40f-428e-d8e2-7af711870b64"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "letras = re.findall(r'\\b[A-zÀ-úü]+\\b', exemplo.lower())\n",
    "letras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wn-sfEMSPW4a"
   },
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords são palavras que, apesar de muito frequentes, não são importantes/relevantes para a máquina. Entre elas, podemos encontrar artigos como “o” e “uma”, ou preposições como “de” e “em”, entre outras palavras frequentes no idioma. Para removê-las do texto, utilizamos uma lista de stopwords disponível na biblioteca NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RngDaWfjPW4f",
    "outputId": "9488d6ba-9fd8-4f4e-b0d0-a720d5ebb486"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "stops #lista de stopwords em inglês "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOwQ1ed6PW4n"
   },
   "source": [
    "Como remover stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWI0dceNPW4p",
    "outputId": "0a03e23d-a82f-43ab-d49f-90f0f6a24846"
   },
   "outputs": [],
   "source": [
    "sem_stopwords = [palavra for palavra in letras if palavra not in stops]\n",
    "palavras_importantes = \" \".join(sem_stopwords)\n",
    "palavras_importantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "afGEg33WPW4z"
   },
   "source": [
    "### Lematização \n",
    "\n",
    "Assim como Stopwords, ter verbos conjugados em um texto não faz diferença quando a máquina vai processá-lo. Por isso, existem duas ferramentas chamadas Lemmatização e Stemmatização. Ambas fazem a mesma coisa: Quando passado um texto como argumento, elas reduzem todas as formas verbais conjugadas à sua raiz. A única diferença, entretanto, é que a função que lemmatiza seu texto reduz todos os verbos a forma verdadeira da raiz  -  por isso quanto maior seu texto, mais tempo essa função demora para rodar no código - , enquanto a função que stemmatiza apenas \"corta\" as palavras no meio usando a raiz como base, o que pode gerar palavras que não existem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGXq4zZiPW40"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spc = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSZ772ZqPW5A",
    "outputId": "6e3bf869-021d-4c0c-c424-32aa8c83d32f"
   },
   "outputs": [],
   "source": [
    "spc_letras = spc(palavras_importantes)\n",
    "lemmas = [token.lemma_ if token.pos_ == 'VERB' else str(token) for token in spc_letras]\n",
    "texto_limpo = \" \".join(lemmas)\n",
    "print(texto_limpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mYlDvyRSPW5q"
   },
   "source": [
    "Construa agora uma função para realizar todos os pré-processamentos ao invés de fazê-los um a um: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_texto(texto):\n",
    "    '''\n",
    "    Função para converter todas as letras para sua forma minúscula, selecionar apenas as letras,\n",
    "    remover stopwords e lematizar o texto. \n",
    "    '''\n",
    "    \n",
    "    ### Transforme as letras para minúscula ###\n",
    "    minusculas = ...\n",
    "    \n",
    "    ### Selecione apenas as letras do texto ##\n",
    "    letras = ... \n",
    "    \n",
    "    ### Removendo as stopwords ###\n",
    "    stops = set(stopwords.words('english')) \n",
    "    # Retire as stopwords de letras\n",
    "    palavras_sem_stopwords = ...\n",
    "    # Junte as palavras sem stopwords \n",
    "    palavras_importantes = ... \n",
    "    \n",
    "    ### Lematização ###\n",
    "    spc_letras = spc(palavras_importantes)\n",
    "    # Lematize o texto \n",
    "    lemmas = ...\n",
    "    # Junte os lemmas \n",
    "    texto_limpo = ...\n",
    "    \n",
    "    return texto_limpo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tCW0gFxwpNPL"
   },
   "source": [
    "Agora vamos aplicá-la aos nossos dados, mais espcificamente na coluna \"letra\", que contém as músicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mrrNlaKPW6V"
   },
   "outputs": [],
   "source": [
    "df['Texto Limpo'] = df['letra'].apply(limpar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bI9AD4--PW6p",
    "outputId": "dd613dbf-904b-43ee-e778-a5cbf10777c3"
   },
   "outputs": [],
   "source": [
    "df.head() # vamos ver como ficou?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o96vg7AZPW61"
   },
   "source": [
    "## Feature Extraction\n",
    "Antes de treinar o nosso modelo, precisamos organizar os nossos documentos em features que o computador consegue entender, assim, vamos precisamos transformar o nosso texto em algum tipo de representação numérica. Para isso, vamos usar o Bag of Words. \n",
    "\n",
    "### Bag of Words \n",
    "**O que é o Bag of Words?:** BoW é uma forma de representação de texto que descreve a ocorrência de palavras em um documento. Para o BoW a ordem não importa, essa forma de representação só se importa se as palavras conhecidas ocorrem ou não no documento (literalmente um \"saco\" de palavras). \n",
    "\n",
    "Para implementarmos o Bag of Words, precisamos de três coisas: \n",
    "1. Um vocabulário com as palavras conhecidas\n",
    "2. A ocorrência dessas palavras\n",
    "3. Formar vetores a partir dos documentos \n",
    "\n",
    "**Exemplo**\n",
    "\n",
    "\"to the left to the left everything you own in the box to the left\"\n",
    "\n",
    "1. Construir o vocabulário\n",
    "\n",
    "    [\"to\", \"the\", \"left\", \"everything\", \"you\", \"own\", \"in\", \"box\"]\n",
    "    \n",
    "\n",
    "2. Ocorrência das palavras\n",
    "\n",
    "    {\"to\": 3, \"the\": 3, \"left\":3, \"everything\":1, \"you\":1, \"own\":1, \"in\":1, \"box\":1}\n",
    "\n",
    "\n",
    "3. Vetores\n",
    "\n",
    "    Considerando que o nosso documento fosse: \"to the left\"\n",
    "\n",
    "    Usando o vocabulário que construímos antes, o nosso vetor seria: \n",
    "\n",
    "    [1, 1, 1, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Ggonk5WPW64"
   },
   "source": [
    "### Count Vectorizer \n",
    "Felizmente, temos o CountVectorizer! Com ele, conseguimos implementar todos os passos acima de uma maneira bem simples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OoiGvcEYPW67"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Bag of words\n",
    "count_vectorizer = CountVectorizer()\n",
    "X = count_vectorizer.fit_transform(df['Texto Limpo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpwp7cETPW7A"
   },
   "source": [
    "Olhando o nosso vocabulário: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vrncvpr2PW7A",
    "outputId": "b52bb5dc-387c-4dea-bc4e-57ed41d7300c"
   },
   "outputs": [],
   "source": [
    "count_vectorizer.get_feature_names() #Todas as palavras do nosso vocabulário "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsoJ7CQkPW7G",
    "outputId": "8fc4c0e3-ee08-4c2c-ed37-c7e7f7d23898"
   },
   "outputs": [],
   "source": [
    "count_vectorizer.vocabulary_.get('love')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mTT5x5WLPW7M"
   },
   "source": [
    "Exemplo da nossa matriz termo-documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XpWLbwtIPW7N",
    "outputId": "4c313714-dbac-4f2d-99b5-2893b8ab92ec"
   },
   "outputs": [],
   "source": [
    "df_cv = pd.DataFrame(X.toarray(), columns = count_vectorizer.get_feature_names())\n",
    "df_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OQasqKkLPW7g"
   },
   "source": [
    "No dataframe acima, cada uma das colunas representa uma das palavras do nosso vocabulário, e cada linha, um dos nossos documentos, ou seja, uma das nossas músicas. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uGmN5MILPW8A"
   },
   "source": [
    "## Separando em Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wieWN86PW8C"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X.toarray()\n",
    "y = df['artista']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V03QPr0QPW8H"
   },
   "source": [
    "## Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJKMuSYLPW8J"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Criando o Modelo Naive Bayes \n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "#.......Treinando o Modelo.......\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "#Fazendo as previsões\n",
    "naive_bayes_pred = naive_bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJNecXsIPW8f"
   },
   "source": [
    "## Métricas \n",
    "Após estarmos com nosso modelo de classificação pronto, devemos avaliá-lo e, para isso, utilizamos as métricas de classificação. \n",
    "### Matriz de confusão\n",
    "Primeiro, quando estamos lidando com um modelo cuja target é categórica (como no nosso caso, em que as músicas pertencem ou a Beyoncé ou a Rihanna), podemos utilizar uma matriz de confusão para analisarmos melhor onde o nosso modelo está acertando e onde ele está errando. Ela apresenta o seguinte formato:\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Fabio_Araujo_Da_Silva/publication/323369673/figure/fig5/AS:597319787479040@1519423543307/Figura-13-Exemplo-de-uma-matriz-de-confusao.png\" alt=\"Exemplo de uma matriz de confusão\"/></a>\n",
    "\n",
    "Na vertical, estão indicados os valores previstos pelo modelo e, na horizontal, os valores reais. Para cada elemento da matriz, temos dois valores associados: o previsto e o real. Se esse valores coincidirem, tem-se uma previsão correta/verdadeira (por exemplo, verdadeiros positivos e verdadeiros negativos, que estão em verde na imagem). Caso contrário, tem-se um erro cometido pelo modelo (como ocorre nos quadrados vermelhos da imagem acima). \n",
    "\n",
    "### Acurácia\n",
    "A acurácia é, basicamente, uma métrica que indica a relação entre quanto o seu modelo acertou do quanto ele avaliou. Considerando a matriz de confusão mostrada, a acurácia seria igual à soma dos verdadeiros positivos com os verdadeiros negativos dividida pelo total (soma dos verdadeiros e falsos positivos e negativos). A acurácia não é uma boa métrica a ser utilizada quando analisamos dados desbalanceados, porque pode acontecer de o modelo prever muito bem o evento mais usual e ser péssimo prevendo o evento raro. Assim, como trata-se de uma média simples de acertos pelo total, a grande quantidade de acertos na previsão do evento mais usual compensaria a baixa taxa de acerto do evento raro, resultando em uma acurácia alta que não reflete corretamente a qualidade de predição do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wi75e7PvPW8g",
    "outputId": "01433652-7bb5-44d7-ecb9-36128ee22e08"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "#Calculando a acurácia\n",
    "acc = accuracy_score(naive_bayes_pred, y_test)\n",
    "\n",
    "#Matriz de confusão \n",
    "cm = confusion_matrix(naive_bayes_pred, y_test)\n",
    "\n",
    "print(\"Acurácia do modelo\", acc)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6mo3KiPPW8w"
   },
   "source": [
    "## Avaliando as músicas\n",
    "Agora, você pode tentar testar o seu modelo com alguma frase e ver a qual cantora ela se assemelha mais: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FjPysbIzPW82",
    "outputId": "d02ed419-177e-44b5-b440-fcf16aa86c01"
   },
   "outputs": [],
   "source": [
    "nova_frase = [\"coloque sua frase aqui\"] \n",
    "teste = count_vectorizer.transform(nova_frase)\n",
    "pred = naive_bayes.predict(teste)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "classificador.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sentimento",
   "language": "python",
   "name": "sentimento"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
